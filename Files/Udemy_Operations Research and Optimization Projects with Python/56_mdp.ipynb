{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Markov Decision Process - MDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Scenario Overview & Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**States:**  \n",
    "$S$: Set of states, indexed by $s$.\n",
    "\n",
    "**Actions:**  \n",
    "$A$: Set of possible actions (prices) that can be applied, indexed by $a$.\n",
    "\n",
    "**Transition Probabilities:**  \n",
    "$P_{s,s'}^a$: Probability of transitioning from state $s$ to state $s'$ under action $a$.\n",
    "\n",
    "**Rewards:**  \n",
    "$R_{s,a}$: Reward received when transitioning from state $s$ to any state under action $a$.\n",
    "\n",
    "**Discount Factor:**  \n",
    "$\\gamma$: The discount factor, where $0 \\leq \\gamma < 1$.\n",
    "\n",
    "**Decision Variables:**  \n",
    "$\\pi(s)$: Policy function, indicating the action to be taken in state $s$.\n",
    "\n",
    "**Objective:**  \n",
    "Maximize the expected total discounted reward over the time horizon $N$:  \n",
    "$\\max_\\pi \\sum_{t=1}^N \\gamma^{t-1} \\sum_{s \\in S} \\sum_{s' \\in S} R_{s,\\pi(s)} P_{s,s'}^{\\pi(s)} V_t(s')$\n",
    "\n",
    "**Constraints:**  \n",
    "1. **Transition Probability Constraints:** For each $s$, $s'$ in $S$ and $a$ in $A$:  \n",
    "$\\sum_{s' \\in S} P_{s,s'}^a = 1$\n",
    "\n",
    "2. **Policy Constraints:** Policy $\\pi(s)$ should be a valid action for each state $s$, typically defined within the action space $A$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Price Levels per State and Time):\n",
      "[[1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [0 0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "# Parameters\n",
    "N = 12  # Time periods\n",
    "S = 5   # Number of states (demand levels)\n",
    "A = 3   # Number of actions (price levels)\n",
    "gamma = 1  # Discount factor\n",
    "\n",
    "# Transition probabilities: P[s, a, s'] = Probability of transition from s to s' given action a\n",
    "P = np.random.rand(S, A, S)  # Example probabilities, should be based on data\n",
    "\n",
    "# Reward function: R[s, a] = Expected revenue for action a in state s\n",
    "R = np.random.rand(S, A)  # Example rewards, should be based on data\n",
    "\n",
    "# Value function and policy initialization\n",
    "V = np.zeros((N+1, S))\n",
    "policy = np.zeros((N, S), dtype=int)\n",
    "\n",
    "# Dynamic programming to solve the MDP\n",
    "for t in range(N-1, -1, -1):\n",
    "    for s in range(S):\n",
    "        Q = np.zeros(A)\n",
    "        for a in range(A):\n",
    "            Q[a] = R[s, a] + gamma * np.sum(P[s, a, :] * V[t+1, :])\n",
    "        V[t, s] = np.max(Q)\n",
    "        policy[t, s] = np.argmax(Q)\n",
    "\n",
    "print(\"Optimal Policy (Price Levels per State and Time):\")\n",
    "print(policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
